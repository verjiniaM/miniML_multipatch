{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scoring training data for miniML. After the plot has been opened, it can be closed at any time and e.g. go to a different starting index. As long as the kernel does not crash, the changes in the scores will be retained and can just be saved at the end."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Applied changes**\n",
    "\n",
    "first round of scoring for nice events (1) and false positives (0), everything else is left in blue\n",
    "second round of scoring - not_so_nice events (1) and noise (0)\n",
    "saved for each file individually in a desginated folder\n",
    "after that each file needs to be moved (manually for now) to the everything_scored folder\n",
    "- otherwise the indexing in the second code cell won't work\n",
    "\n",
    "script to go thourt all and count\n",
    "script to collect all all categories and run though them once again\n",
    "script to collected all the category separated files into one complete dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from ScoringPanel import ScoringPanel\n",
    "import numpy as np\n",
    "\n",
    "plt.style.use('default')\n",
    "plt.rcParams['figure.facecolor'] = 'white'\n",
    "plt.rcParams['axes.facecolor'] = 'white'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_folder = '/Users/verjim/miniML_multipatch/model_training/extract_training_data/output/Oct_2024_train/'\n",
    "all_files = os.listdir(output_folder)\n",
    "h5s_ = sorted([x for x in all_files if x[-3:] == '.h5'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "j = 0\n",
    "\n",
    "i = h5s_[j]\n",
    "# Load data to be scored\n",
    "file_path = output_folder + i\n",
    "with h5py.File(file_path, 'r') as f:\n",
    "    x = f['events'][:]\n",
    "    y = f['scores'][:]\n",
    "\n",
    "print(i)\n",
    "print(x.shape, y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open the interactive plot for scoring.\n",
    "- To go to the next/previous event, use forward/backward arrows on the keyboard or the scroll wheel.\n",
    "- To change the score, press 'm' on the keyboard.\n",
    "\n",
    "\n",
    "The scores mean the following:\n",
    "- 0: red, not an event of interest\n",
    "- 1: black, event of interest\n",
    "- 2: blue, unclear\n",
    "\n",
    "For training the model, there should be no events with scores 2 in the dataset. We added this so one can mark them and either exclude them or come back to them at a later time point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib qt\n",
    "# Open plot for scoring\n",
    "fig, ax = plt.subplots(1, 1)\n",
    "tracker = ScoringPanel(fig, ax, x, y, start_ind=0)\n",
    "fig.canvas.mpl_connect('key_press_event', tracker.onclick)\n",
    "fig.canvas.mpl_connect('scroll_event', tracker.onscroll)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "false_positives = x[np.where(y==0)].copy()\n",
    "false_positives_score = y[y == 0].copy()\n",
    "\n",
    "okey_events = x[np.where(y==1)].copy()\n",
    "okey_events_score = y[y == 1].copy()\n",
    "\n",
    "# nice_events = x[np.where(y==1)].copy()\n",
    "# nice_events_score = y[y == 1].copy()\n",
    "\n",
    "# unscored_x = x[np.where(y==2)].copy()  \n",
    "# unscored = y[y == 2].copy()\n",
    "\n",
    "# if len(false_positives) + len(nice_events) + len(unscored) == len(y):\n",
    "#     print('free to continue')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Save modified dataset. Changes will only be saved after running this cell.\n",
    "\n",
    "with h5py.File(output_folder + 'false_positives/' +  i[:-3] + 'false_pos.h5', 'w') as f:\n",
    "    f.create_dataset(\"events\", data = false_positives)\n",
    "    f.create_dataset(\"scores\", data = false_positives_score)\n",
    "\n",
    "\n",
    "with h5py.File(output_folder + 'not_so_nice_events/' + i[:-3] + '_okey.h5', 'w') as f:\n",
    "    f.create_dataset(\"events\", data = okey_events)\n",
    "    f.create_dataset(\"scores\", data = okey_events_score)\n",
    "\n",
    "# with h5py.File(output_folder + 'nice_events/' + i[:-3] + '_nice.h5', 'w') as f:\n",
    "#     f.create_dataset(\"events\", data = nice_events)\n",
    "#     f.create_dataset(\"scores\", data = nice_events_score)\n",
    "\n",
    "# with h5py.File(output_folder + 'unscored_round_1/' + i[:-3] + '_unscored.h5', 'w') as f:\n",
    "#     f.create_dataset(\"events\", data = unscored_x)\n",
    "#     f.create_dataset(\"scores\", data = unscored)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the previously unscored, extract not so nice/ clear events and noise\n",
    "\n",
    "The scores mean the following:\n",
    "- 0: red, noise\n",
    "- 1: black, not so nice event\n",
    "- 2: blue, unclear, delete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib qt\n",
    "# Open plot for scoring\n",
    "fig, ax = plt.subplots(1, 1)\n",
    "tracker = ScoringPanel(fig, ax, unscored_x, unscored, start_ind=0)\n",
    "fig.canvas.mpl_connect('key_press_event', tracker.onclick)\n",
    "fig.canvas.mpl_connect('scroll_event', tracker.onscroll)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise = unscored_x[np.where(unscored==0)].copy()\n",
    "noise_score = unscored[unscored == 0].copy()\n",
    "okey_events = unscored_x[np.where(unscored==1)].copy()\n",
    "okey_events_score = unscored[unscored == 1].copy()\n",
    "\n",
    "rest_x = unscored_x[np.where(unscored==2)].copy()  \n",
    "rest = unscored[unscored == 2].copy()\n",
    "\n",
    "if len(noise) + len(okey_events) + len(rest) == len(unscored):\n",
    "    print('free to continue')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File(output_folder + 'noise/' +  i[:-3] + 'noise.h5', 'w') as f:\n",
    "    f.create_dataset(\"events\", data = noise)\n",
    "    f.create_dataset(\"scores\", data = noise_score)\n",
    "\n",
    "with h5py.File(output_folder + 'not_so_nice_events/' + i[:-3] + '_okey.h5', 'w') as f:\n",
    "    f.create_dataset(\"events\", data = okey_events)\n",
    "    f.create_dataset(\"scores\", data = okey_events_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# move the file to the scored folder\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collecting all the scored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "type = 'false_positives/' #not_so_nice_events/, 'false_positives/'\n",
    "\n",
    "output_folder = '/Users/verjim/miniML_multipatch/model_training/extract_training_data/output/Oct_2024_train/' + type\n",
    "all_files = os.listdir(output_folder)\n",
    "\n",
    "# all_files = sorted([x for x in all_files if x[-12:] == 'datanoise.h5'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = 0\n",
    "for fn in all_files:\n",
    "    file_path = output_folder + fn\n",
    "    with h5py.File(file_path, 'r') as f:\n",
    "        x = f['events'][:]\n",
    "        y = f['scores'][:]\n",
    "    counter += len(x)\n",
    "\n",
    "print(counter)\n",
    "\n",
    "# to 12: FP - 564, nice - 308, noise - 93, not_so_nice - 334"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Count for each event type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "type = 'nice_events/' # 'not_so_nice_events/', 'false_positives/', 'noise/', 'nice_events/'\n",
    "\n",
    "scores_dict = {'false_positives/': 0, 'not_so_nice_events/': 1 , 'noise/': 0,'nice_events/': 1}\n",
    "\n",
    "output_folder = '/Users/verjim/miniML_multipatch/model_training/extract_training_data/output/Oct_2024_train/'\n",
    "all_files = os.listdir(output_folder + type)\n",
    "\n",
    "if '.DS_Store' in all_files:\n",
    "    all_files.remove('.DS_Store')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, file in enumerate(all_files):\n",
    "    fn = output_folder + type + file\n",
    "    count = 0\n",
    "    if i == 0:\n",
    "        with h5py.File(fn, 'r') as f:\n",
    "            x = f['events'][:][:, :750]\n",
    "            y = f['scores'][:]\n",
    "    else:\n",
    "        with h5py.File(fn, 'r') as f:\n",
    "            x = np.concatenate((x, f['events'][:][:, :750]))\n",
    "            y = np.concatenate((y, f['scores'][:]), axis = None)\n",
    "\n",
    "print(x.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib qt\n",
    "# Open plot for scoring\n",
    "fig, ax = plt.subplots(1, 1)\n",
    "tracker = ScoringPanel(fig, ax, x, y, start_ind = 0)\n",
    "fig.canvas.mpl_connect('key_press_event', tracker.onclick)\n",
    "fig.canvas.mpl_connect('scroll_event', tracker.onscroll)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = scores_dict[type]\n",
    "\n",
    "events_keep = x[np.where(y==score)].copy()\n",
    "scores_keep = y[y == score].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File(output_folder + type[:-1] + '_complete.h5', 'w') as f:\n",
    "    f.create_dataset(\"events\", data = events_keep)\n",
    "    f.create_dataset(\"scores\", data = scores_keep)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collect all into one dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_folder = '/Users/verjim/miniML_multipatch/model_training/extract_training_data/output/Oct_2024_train/'\n",
    "all_files = os.listdir(output_folder)\n",
    "\n",
    "collected = [x for x in all_files if x[-3:] == '.h5']\n",
    "\n",
    "dict_nums = {'false': 500, 'not_s': 300, 'noise': 100, 'nice_': 300}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(x[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, file in enumerate(collected):\n",
    "    fn = output_folder + file\n",
    "    count = dict_nums[file[:5]]\n",
    "    if i == 0:\n",
    "        with h5py.File(fn, 'r') as f:\n",
    "            x = f['events'][:count]\n",
    "            y = f['scores'][:count]\n",
    "    else:\n",
    "        with h5py.File(fn, 'r') as f:\n",
    "            x = np.concatenate((x, f['events'][:count]))\n",
    "            y = np.concatenate((y, f['scores'][:count]), axis = None)\n",
    "\n",
    "print(x.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File(output_folder + 'complete_training_dataset.h5', 'w') as f:\n",
    "    f.create_dataset(\"events\", data = x)\n",
    "    f.create_dataset(\"scores\", data = y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "miniML_clean",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
